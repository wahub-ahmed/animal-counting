{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# CIS 519 Final Project: Counting Animals in a Sequence of Images\n","## Wahub Ahmed, Ankith Pinnamaneni, Bailey Hirota\n","#### Adapted from https://www.kaggle.com/code/jordandahan/iwild2022"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T20:02:06.120283Z","iopub.status.busy":"2023-04-26T20:02:06.119772Z","iopub.status.idle":"2023-04-26T20:02:15.731622Z","shell.execute_reply":"2023-04-26T20:02:15.730716Z","shell.execute_reply.started":"2023-04-26T20:02:06.120246Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorFlow version: 2.6.4\n","Is GPU available? tf.test.is_gpu_available: False\n"]}],"source":["import argparse\n","import glob\n","import os\n","import sys\n","import time\n","import warnings\n","import json, codecs\n","import random\n","import matplotlib.pyplot as plt\n","from scipy.optimize import linear_sum_assignment as linear_assignment\n","import math\n","\n","import cycler\n","from matplotlib import colors\n","from matplotlib import pyplot as plt\n","import matplotlib.patches as patches\n","import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","from PIL import Image, ImageFile, ImageFont, ImageDraw\n","import statistics\n","from torch.utils.data import Dataset,DataLoader\n","import torch\n","import torchvision\n","import cv2\n","from tqdm import tqdm\n","%matplotlib inline\n","\n","# from CameraTraps.ct_utils import truncate_float\n","print('TensorFlow version:', tf.__version__)\n","print('Is GPU available? tf.test.is_gpu_available:', tf.test.is_gpu_available())"]},{"cell_type":"markdown","metadata":{},"source":["## Load metadata information"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-20T19:22:48.045378Z","iopub.status.busy":"2023-04-20T19:22:48.044305Z","iopub.status.idle":"2023-04-20T19:22:56.064840Z","shell.execute_reply":"2023-04-20T19:22:56.063835Z","shell.execute_reply.started":"2023-04-20T19:22:48.045338Z"},"trusted":true},"outputs":[],"source":["def image_name_to_id(name):\n","    return name.rstrip('.jpg')\n","\n","def read_image(path):\n","    with tf.io.gfile.GFile(path, 'rb') as f:\n","        return np.array(Image.open(f))\n","    \n","def read_json(path):\n","    with tf.io.gfile.GFile(path) as f:\n","        return json.load(f)\n","    \n","def create_detection_map(annotations,mode=\"train\"):\n","    \"\"\"Creates a dict mapping IDs ---> detections.\"\"\"\n","\n","    ann_map = {}\n","    for image in annotations['images']:\n","        if image['file'].split('/')[0] == mode:\n","            ann_map[image['file'].split('/')[-1].rstrip('.jpg')] = image['detections']\n","    return ann_map\n","\n","# TRAIN\n","IMAGES_DIR_TRAIN = \"/kaggle/input/iwildcam2022-fgvc9/train/train\"\n","BOX_ANNOTATION_FILE = \"/kaggle/input/iwildcam2022-fgvc9/metadata/metadata/iwildcam2022_mdv4_detections.json\"\n","MASKS_DIR = \"/kaggle/input/iwildcam2022-fgvc9/instance_masks/instance_masks\"\n","\n","images_train = tf.io.gfile.listdir(IMAGES_DIR_TRAIN)\n","# The annotations file contains annotations for all images in train and test\n","annotations = read_json(BOX_ANNOTATION_FILE)\n","detection_train_map = create_detection_map(annotations)\n","images_train_ids = list(detection_train_map.keys())\n","\n","#TEST\n","IMAGES_DIR_TEST = \"/kaggle/input/iwildcam2022-fgvc9/test/test\"\n","images_test = tf.io.gfile.listdir(IMAGES_DIR_TEST)\n","detection_test_map = create_detection_map(annotations,mode=\"test\")\n","images_test_ids = list(detection_test_map.keys())\n","\n","print(f'length of detection map for train = {len(detection_train_map)}\\nlength of detection map for test = {len(detection_test_map)}\\n')\n","print(f'length of images for train = {len(images_train)}\\nlength of images for test = {len(images_test)}\\n')\n","print(f'total annotations from megaDetector model = {len(annotations[\"images\"])}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Data Augmentation Method"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def read_image(path):\n","    with tf.io.gfile.GFile(path, 'rb') as f:\n","        return np.array(Image.open(f).convert('L')) # Change 'L' to '1' and add parameter dithering = Image.FLOYDSTEINBERG"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-20T19:22:56.066544Z","iopub.status.busy":"2023-04-20T19:22:56.066098Z","iopub.status.idle":"2023-04-20T19:22:58.754619Z","shell.execute_reply":"2023-04-20T19:22:58.753522Z","shell.execute_reply.started":"2023-04-20T19:22:56.066470Z"},"trusted":true},"outputs":[],"source":["with codecs.open(\"../input/iwildcam2022-fgvc9/metadata/metadata/iwildcam2022_train_annotations.json\", 'r',\n","                 encoding='utf-8', errors='ignore') as f:\n","    train_meta = json.load(f)\n","    \n","with codecs.open(\"../input/iwildcam2022-fgvc9/metadata/metadata/iwildcam2022_test_information.json\", 'r',\n","                 encoding='utf-8', errors='ignore') as f:\n","    test_meta = json.load(f)\n","seq_test = pd.DataFrame(test_meta['images'])\n","#train_cat.columns = [ 'category_id', 'scientificName','family', 'genus']\n","display(seq_test)\n","seq_train = pd.DataFrame(train_meta['images'])\n","#train_cat.columns = [ 'category_id', 'scientificName','family', 'genus']\n","display(seq_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Show & Analyze Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-20T19:22:58.757100Z","iopub.status.busy":"2023-04-20T19:22:58.756715Z","iopub.status.idle":"2023-04-20T19:23:03.849525Z","shell.execute_reply":"2023-04-20T19:23:03.848663Z","shell.execute_reply.started":"2023-04-20T19:22:58.757067Z"},"trusted":true},"outputs":[],"source":["COLOR_CYCLER = cycler.cycler(color=['tab:blue', 'tab:green', 'tab:orange',\n","                                    'tab:red', 'tab:purple'])\n","pd_example = seq_train[seq_train['seq_id'] == \"30048d32-7d42-11eb-8fb5-0242ac1c0002\"]\n","def  get_image_annotation(image, detection_annotations, categories,instance_id_image,ax):\n","    \"\"\"Plot boxes and mask annotations for a given image.\n","\n","            Args:\n","            image: An image array of shape [H, W, 3]\n","            detection_annotations: A list of detections. Each detection is a dict\n","              containing the keys 'category', 'bbox' and 'conf'.\n","            categories: A dict mapping category IDs to names.\n","            instance_id_image: An array of shape [H, W] containing the instance ID\n","              at each pixel. IDs are expected to be 1-indexed, with 0 reserved for\n","              the background.\"\"\"\n","        \n","    cycle_iter = COLOR_CYCLER()\n","    image_height, image_width = image.shape[:2]\n","    ax.imshow(image)\n","    for i, annotation in enumerate(detection_annotations):\n","        xmin, ymin, width, height = annotation['bbox']\n","        xmin *= image_width\n","        ymin *= image_height\n","        width *= image_width\n","        height *= image_height\n","        color = next(cycle_iter)['color']\n","        rect = patches.Rectangle((xmin, ymin), width, height,\n","                                 linewidth=3, edgecolor=color, facecolor='none')\n","        ax.add_patch(rect)\n","        label = '{}:{:.2f}'.format(categories[annotation['category']],\n","                                   annotation['conf'])\n","        ax.text(xmin, ymin - 5, label, fontsize=30, color='white',\n","                  bbox=dict(boxstyle='square,pad=0.0', facecolor=color, alpha=0.75,\n","                            ec='none'))\n","        r, g, b, _ = colors.to_rgba(color)\n","        color_array = np.array([r, g, b]).reshape(1, 1, 3)\n","        color_image = np.ones((image_height, image_width, 3)) * color_array\n","        mask = (instance_id_image == (i + 1)).astype(np.float32)[:, :, np.newaxis]\n","        color_mask = np.concatenate([color_image, mask], axis=2)\n","        \n","        ax.imshow(color_mask, alpha=0.5)\n","    return color_mask\n","        \n","def show_images_seq(data,detections,train = True):\n","    rows = data.shape[0]\n","    cols = data.shape[1]\n","    fig, axs = plt.subplots(rows, dpi=80,  figsize=(rows*5, rows*5))\n","    for i in range(rows):\n","        image_name = data[\"file_name\"][i]\n","        image_path = os.path.join(IMAGES_DIR_TRAIN if train else IMAGES_DIR_TEST, image_name)\n","        image_id = image_name_to_id(image_name)\n","        mask_path = os.path.join(MASKS_DIR, f'{image_id}.png')\n","        image = read_image(image_path)\n","        if (image_id not in detections) or (len(detections[image_id]) == 0) or ( not tf.io.gfile.exists(mask_path)):\n","            plt.title(f'{image_name} missing detection data.')\n","            axs[i].imshow(image)\n","        else:\n","            detection_annotations = detections[image_id]\n","            instance_id_image = read_image(mask_path)\n","            image = get_image_annotation(image,detection_annotations,annotations['detection_categories'],instance_id_image,axs[i])   \n","            \n","        axs[i].axis(\"off\")    \n","    plt.show()\n","            \n","show_images_seq(pd_example,detection_train_map)"]},{"cell_type":"markdown","metadata":{},"source":["## Solutions\n","* Solution 1: Iwildcam competition winners in 2021 counted the maximum number of bboxes per frame as part of their sequence.\n","* Solution 2. The identified objects will be tracked using an object tracker, which gives each object a unique ID number."]},{"cell_type":"markdown","metadata":{},"source":["### Solution 1: Maximum detections in sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-20T19:23:03.851492Z","iopub.status.busy":"2023-04-20T19:23:03.850542Z","iopub.status.idle":"2023-04-20T19:23:03.859802Z","shell.execute_reply":"2023-04-20T19:23:03.858321Z","shell.execute_reply.started":"2023-04-20T19:23:03.851444Z"},"trusted":true},"outputs":[],"source":["def count_detections(row,detections, thres):\n","    image_id = row['file_name'].split('.')[0]\n","    threshold = thres\n","    count = 0\n","    \n","    for bbox in detections[image_id]:\n","        if bbox['conf'] > threshold:\n","            count += 1   \n","    return count\n","\n","def generate_zero_submission(seq_ids):\n","    sub = pd.DataFrame(seq_ids, columns=['Id'])\n","    sub['Predicted'] = 0\n","    return sub\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-20T19:23:03.862242Z","iopub.status.busy":"2023-04-20T19:23:03.861756Z","iopub.status.idle":"2023-04-20T19:24:53.274180Z","shell.execute_reply":"2023-04-20T19:24:53.272899Z","shell.execute_reply.started":"2023-04-20T19:23:03.862176Z"},"trusted":true},"outputs":[],"source":["thres_list = [0.8, 0.85, 0.9, 0.95, 0.97]\n","for thres in thres_list: \n","    seq_test[\"detections_count\"] = np.nan\n","    for idx,row in tqdm(seq_test.iterrows()):\n","        seq_test.at[idx, 'detections_count'] = count_detections(row,detection_test_map, thres)\n","\n","    submission_res_by_max = generate_zero_submission(seq_test.seq_id.unique())\n","    for seq_id in tqdm(seq_test.seq_id.unique()):\n","        max_count = seq_test[seq_test.seq_id == seq_id]['detections_count'].max()\n","        submission_res_by_max.loc[submission_res_by_max.Id == seq_id, 'Predicted'] = max_count\n","        \n","    display(submission_res_by_max)\n","    print(\"Final results:\")\n","    print(max(submission_res_by_max.Predicted))\n","    submission_res_by_max.to_csv (r'res_by_max_'+str(thres)+'.csv', index = False, header=True) \n"]},{"cell_type":"markdown","metadata":{},"source":["### Solution 2: using Object-Tracker-Model\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Intro to Tracking\n","Tracking in deep learning is the task of predicting the positions of objects throughout a video using their spatial as well as temporal features. More technically, Tracking is getting the initial set of detections, assigning unique ids, and tracking them throughout frames of the video (or sequence of frames) feed while maintaining the assigned ids. Tracking is generally a two-step process:  \n","    **1. A detection module for target localization: The module responsible for detecting and localization of the object in the frame using some object detector like YOLOv4, CenterNet, etc (In our case- the detections is given part of db).**  \n","    **2. A motion predictor: This module is responsible for predicting the future motion of the object using its past information.**\n","\n","### Types of Trackers\n","\n","1. **Single Object Tracker**- These types of trackers track only a single object even if there are many other objects present in the frame. (We will not use here).\n","2. **Multiple Object Tracker -**\n","These types of trackers can track multiple objects present in a frame. Some of the algorithms include DeepSORT, JDE, and CenterTrack which are very powerful algorithms and handle most of the challenges faced by trackers. (We will present the idea below).  \n","\n","DeepSORT: [arXiv:1703.07402](https://arxiv.org/abs/1703.07402)  \n","Centroid: [centroid tracking](https://pyimagesearch.com/2018/07/23/simple-object-tracking-with-opencv/)"]},{"cell_type":"markdown","metadata":{},"source":["#### Centroid-Tracker using L2 Distance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-20T19:24:53.290138Z","iopub.status.busy":"2023-04-20T19:24:53.289631Z","iopub.status.idle":"2023-04-20T19:27:22.626389Z","shell.execute_reply":"2023-04-20T19:27:22.625272Z","shell.execute_reply.started":"2023-04-20T19:24:53.290094Z"},"trusted":true},"outputs":[],"source":["                                                                                                                               #######  \n","class EuclideanDistTracker:\n","    def __init__(self):\n","        self.center_points = {}\n","        self.id_count = 0\n","    \n","    def update(self, objects_rect):\n","        \"\"\"\n","        Parameters:\n","        -----------\n","        object_rect:  array of bounding box coordinates.\n","        --------\n","        Returns:\n","            list containing [x,y,w,h,object_id].\n","                x,y,w,h are the bounding box coordinates, and object_id is the id assigned to that particular bounding box.\n","        --------\n","        \"\"\"\n","        # Objects boxes and ids\n","        objects_bbs_ids = []\n","\n","        # Get center point of new object\n","        for rect in objects_rect:\n","            x, y, w, h = rect\n","            cx,cy = (x+w)/2, (y+h)/2\n","#             cx = (x + x + w) // 2 # Center x\n","#             cy = (y + y + h) // 2 # Center y\n","            # Find out if that object was detected already\n","            same_object_detected = False\n","            for id, pt in self.center_points.items():\n","                dist = math.hypot(cx - pt[0], cy - pt[1])\n","\n","                if dist < 25:\n","                    self.center_points[id] = (cx, cy)\n","                    objects_bbs_ids.append([x, y, w, h, id])\n","                    same_object_detected = True\n","                    break\n","\n","            # New object is detected we assign the ID to that object\n","            if same_object_detected is False:\n","                self.center_points[self.id_count] = (cx, cy)\n","                objects_bbs_ids.append([x, y, w, h, self.id_count])\n","                self.id_count += 1\n","\n","        # Clean the dictionary by center points to remove IDS not used anymore\n","        new_center_points = {}\n","        for obj_bb_id in objects_bbs_ids:\n","            _, _, _, _, object_id = obj_bb_id\n","            center = self.center_points[object_id]\n","            new_center_points[object_id] = center\n","\n","        # Update dictionary with IDs not used removed\n","        self.center_points = new_center_points.copy()\n","        return objects_bbs_ids\n","    \n","def get_detections(detections_dict,height,width, thres):\n","    detections = []\n","    for diction in detections_dict:\n","        if diction['conf'] > thres:\n","            detections.append(diction['bbox'])\n","\n","    return detections \n","\n","thres_list = [0.8, 0.85, 0.9, 0.95, 0.97]\n","for thres in thres_list: \n","    tracker = EuclideanDistTracker()\n","    submission_res_by_tracks = generate_zero_submission(seq_test.seq_id.unique())\n","    i=0\n","    for seq_id in tqdm(seq_test.seq_id.unique()):\n","        tracker.__init__()\n","        seq_frames = seq_test[seq_test.seq_id == seq_id].sort_values(by=['seq_frame_num']).reset_index( drop = True)\n","        for _,frame in seq_frames.iterrows():\n","            detections = get_detections(detection_test_map[frame['id']], frame['height'],frame['width'], thres)\n","            boxes_ids = tracker.update(detections)\n","        if i < 3 and tracker.id_count > 1 :\n","            display(seq_frames)\n","            show_images_seq(seq_frames,detection_test_map,train=False)\n","            print(f'id_count = {tracker.id_count}')\n","            i+=1\n","            \n","        submission_res_by_tracks.loc[submission_res_by_tracks.Id == seq_id, 'Predicted'] = tracker.id_count\n","\n","    display(submission_res_by_tracks)\n","    print(max(submission_res_by_tracks.Predicted))\n","    submission_res_by_tracks.to_csv (r'res_by_tracks_' + str(thres) + '.csv', index = False, header=True) \n"]},{"cell_type":"markdown","metadata":{},"source":["#### Centroid-Tracker using L1 (Manhattan) Distance"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-20T19:27:22.628386Z","iopub.status.busy":"2023-04-20T19:27:22.627780Z","iopub.status.idle":"2023-04-20T19:29:51.949257Z","shell.execute_reply":"2023-04-20T19:29:51.948050Z","shell.execute_reply.started":"2023-04-20T19:27:22.628352Z"},"trusted":true},"outputs":[],"source":["class L1DistTracker:\n","    def __init__(self):\n","        self.center_points = {}\n","        self.id_count = 0\n","    \n","    def update(self, objects_rect):\n","        \"\"\"\n","        Parameters:\n","        -----------\n","        object_rect:  array of bounding box coordinates.\n","        --------\n","        Returns:\n","            list containing [x,y,w,h,object_id].\n","                x,y,w,h are the bounding box coordinates, and object_id is the id assigned to that particular bounding box.\n","        --------\n","        \"\"\"\n","        # Objects boxes and ids\n","        objects_bbs_ids = []\n","\n","        # Get center point of new object\n","        for rect in objects_rect:\n","            x, y, w, h = rect\n","            cx,cy = x+w/2, y+h/2\n","            # Find out if that object was detected already\n","            same_object_detected = False\n","            for id, pt in self.center_points.items():\n","                dist = abs(cx - pt[0]) + abs(cy - pt[1])\n","\n","                if dist < 25:\n","                    self.center_points[id] = (cx, cy)\n","                    objects_bbs_ids.append([x, y, w, h, id])\n","                    same_object_detected = True\n","                    break\n","\n","            # New object is detected we assign the ID to that object\n","            if same_object_detected is False:\n","                self.center_points[self.id_count] = (cx, cy)\n","                objects_bbs_ids.append([x, y, w, h, self.id_count])\n","                self.id_count += 1\n","\n","        # Clean the dictionary by center points to remove IDS not used anymore\n","        new_center_points = {}\n","        for obj_bb_id in objects_bbs_ids:\n","            _, _, _, _, object_id = obj_bb_id\n","            center = self.center_points[object_id]\n","            new_center_points[object_id] = center\n","\n","        # Update dictionary with IDs not used removed\n","        self.center_points = new_center_points.copy()\n","        return objects_bbs_ids\n","    \n","def get_detections(detections_dict, height, width, thres):\n","    detections = []\n","    for diction in detections_dict:\n","        if diction['conf'] > thres:\n","            detections.append(diction['bbox'])\n","\n","    return detections \n","\n","thres_list = [0.8, 0.85, 0.9, 0.95, 0.97]\n","\n","for thres in thres_list: \n","    tracker = L1DistTracker()\n","    submission_res_by_tracks = generate_zero_submission(seq_test.seq_id.unique())\n","    i=0\n","    for seq_id in tqdm(seq_test.seq_id.unique()):\n","        tracker.__init__()\n","        seq_frames = seq_test[seq_test.seq_id == seq_id].sort_values(by=['seq_frame_num']).reset_index(drop=True)\n","        for _,frame in seq_frames.iterrows():\n","            detections = get_detections(detection_test_map[frame['id']], frame['height'], frame['width'], thres)\n","            boxes_ids = tracker.update(detections)\n","        if i < 3 and tracker.id_count > 1 :\n","            display(seq_frames)\n","            show_images_seq(seq_frames, detection_test_map, train=False)\n","            print(f'id_count = {tracker.id_count}')\n","            i += 1\n","            \n","        submission_res_by_tracks.loc[submission_res_by_tracks.Id == seq_id, 'Predicted'] = tracker.id_count\n","\n","    display(submission_res_by_tracks)\n","    print(max(submission_res_by_tracks.Predicted))\n","    submission_res_by_tracks.to_csv(r'res_by_tracks_l1_' + str(thres) + '.csv', index=False, header=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Faster R-CNN Implementation Using Pretrained Model\n","### Load model and define bounding box heuristics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-26T20:02:30.219675Z","iopub.status.busy":"2023-04-26T20:02:30.219046Z","iopub.status.idle":"2023-04-26T20:02:51.282575Z","shell.execute_reply":"2023-04-26T20:02:51.280869Z","shell.execute_reply.started":"2023-04-26T20:02:30.219641Z"},"trusted":true},"outputs":[],"source":["# Load the pretrained model\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Define a function to draw bounding boxes on an image\n","def draw_boxes(image, boxes, labels, scores):\n","    # Convert the image to BGR format for OpenCV\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","    # Loop over the boxes\n","    for box, label, score in zip(boxes, labels, scores):\n","        # Draw a rectangle around the box\n","        cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n","        # Draw the label and score on the top-left corner of the box\n","        cv2.putText(image, f\"{label}: {score:.2f}\", (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n","    # Return the image with boxes\n","    return image\n","\n","# Define a function to count the maximum number of bounding boxes in a sequence of images\n","def count_max_boxes(images):\n","    # Initialize a list to store the number of boxes for each image\n","    num_boxes = []\n","    # Loop over the images\n","    for image in images:\n","        # Convert the image to RGB format for PyTorch\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        # Convert the image to a tensor and normalize it\n","        image_tensor = torchvision.transforms.functional.to_tensor(image)\n","        image_tensor = torchvision.transforms.functional.normalize(image_tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","        # Add a batch dimension to the image tensor\n","        image_tensor = image_tensor.unsqueeze(0)\n","        # Get the predictions from the model\n","        with torch.no_grad():\n","            predictions = model(image_tensor)\n","        # Get the boxes, labels and scores from the predictions\n","        boxes = predictions[0][\"boxes\"].numpy()\n","        labels = predictions[0][\"labels\"].numpy()\n","        scores = predictions[0][\"scores\"].numpy()\n","        # Filter out the boxes with low scores\n","        threshold = 0.95\n","        mask = scores >= threshold\n","        boxes = boxes[mask]\n","        labels = labels[mask]\n","        scores = scores[mask]\n","        # Append the number of boxes to the list\n","        num_boxes.append(len(boxes))\n","    # Return the maximum number of boxes in the list\n","    return max(num_boxes)\n","\n","# Define the paths for the training and testing images\n","train_path = \"/kaggle/input/iwildcam2022-fgvc9/train/train\"\n","test_path = \"/kaggle/input/iwildcam2022-fgvc9/test/test\"\n","\n","# Get a list of training and testing images\n","train_images = glob.glob(os.path.join(train_path, \"*.jpg\"))\n","test_images = glob.glob(os.path.join(test_path, \"*.jpg\"))\n","\n","# Load a sample training image and get its predictions\n","sample_train_image = cv2.imread(train_images[0])\n","sample_train_predictions = model(torchvision.transforms.functional.to_tensor(sample_train_image).unsqueeze(0))"]},{"cell_type":"markdown","metadata":{},"source":["### Draw bounding boxes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-04-26T20:01:32.874521Z","iopub.status.idle":"2023-04-26T20:01:32.874947Z","shell.execute_reply":"2023-04-26T20:01:32.874768Z","shell.execute_reply.started":"2023-04-26T20:01:32.874750Z"},"trusted":true},"outputs":[],"source":["# Draw bounding boxes on the sample training image and display it\n","sample_train_boxes = sample_train_predictions[0][\"boxes\"].detach().numpy()\n","sample_train_labels = sample_train_predictions[0][\"labels\"].detach().numpy()\n","sample_train_scores = sample_train_predictions[0][\"scores\"].detach().numpy()\n","sample_train_image_with_boxes = draw_boxes(sample_train_image, sample_train_boxes, sample_train_labels, sample_train_scores)\n","plt.imshow(sample_train_image_with_boxes)\n","plt.show()\n","plt.close()\n","\n","# Load a sample testing image and get its predictions\n","sample_test_image = cv2.imread(test_images[0])\n","sample_test_predictions = model(torchvision.transforms.functional.to_tensor(sample_test_image).unsqueeze(0))\n","\n","# Draw bounding boxes on the sample testing image and display it\n","sample_test_boxes = sample_test_predictions[0][\"boxes\"].detach().numpy()\n","sample_test_labels = sample_test_predictions[0][\"labels\"].detach().numpy()\n","sample_test_scores = sample_test_predictions[0][\"scores\"].detach().numpy()\n","sample_test_image_with_boxes = draw_boxes(sample_test_image, sample_test_boxes, sample_test_labels, sample_test_scores)\n","plt.imshow(sample_test_image_with_boxes)\n","plt.show()\n","plt.close()\n","\n","# Load a sequence of training images and count the maximum number of bounding boxes\n","sequence_train_images = [cv2.imread(image) for image in train_images[:10]] # You can change the number of images as per your preference\n","max_train_boxes = count_max_boxes(sequence_train_images)\n","print(f\"The maximum number of bounding boxes in the sequence of training images is {max_train_boxes}\")\n","\n","# Load a sequence of testing images and count the maximum number of bounding boxes\n","sequence_test_images = [cv2.imread(image) for image in test_images[:10]] # You can change the number of images as per your preference\n","max_test_boxes = count_max_boxes(sequence_test_images)\n","print(f\"The maximum number of bounding boxes in the sequence of testing images is {max_test_boxes}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion\n","* It is obvious that the performance of the object tracker is not as good as the counting heuristic. There could be several reasons for this.\n","- Also, in order to get better performance - we can produce the identifications ourselves by our own model.\n","\n","    * It makes sense that the counting heuristic works best because we are counting animals in this project and since they were taken with a cameratrap (which takes many pictures in sequence when it detects movement) it could be that in one frame all the animals that passed by in sequence were captured. Therefore, the simple and working solution is to count the maximum number of identifications in the sequence of images.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
